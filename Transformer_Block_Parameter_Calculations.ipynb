{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Block Parameter Calculations.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK-6k-dYg4dT",
        "outputId": "b997149e-b055-4013-ca11-092d3a01bc4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)          [(None, 145, 64)]    0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 145, 64)     128         ['input_10[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 145, 64)     66368       ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 145, 64)      0           ['input_10[0][0]',               \n",
            "                                                                  'multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 145, 64)     128         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 145, 128)     8320        ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 145, 128)     0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 145, 64)      8256        ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 145, 64)      0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 145, 64)      0           ['add_6[0][0]',                  \n",
            "                                                                  'dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 83,200\n",
            "Trainable params: 83,200\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "### This notebook checks how many parameters in the transformer block\n",
        "### Notes:\n",
        "### 1. Attention learnable parameters calculations\n",
        "### key_dim as the dimension where each key, query & value is projected to for each head\n",
        "### so if model input has a vector of 64 elements & layer parameter  key_dim = 64 ==> \n",
        "### we need to project each sequence vector to 3 * 64 vecor for each head\n",
        "### ==> (64*64 weights + 64 bias for each projection) * 3 (for mini Q,K,V vectors) projections * 4 (number of heads)\n",
        "### leading to 49,920 learnable parameters\n",
        "### This outputs 4 (attention heads) * (mini-attention vectors each of 64 elements)\n",
        "### The aattention vectors from each head (64 each matching the projection size of the value vectors) \n",
        "### are concatenated leading to one attention vector of 256 elements\n",
        "### This vector is projected to 64 element leading to (256 * 64 + 64) = 16,448 learnable parameters\n",
        "### this leaves us with 49,920+16448 = 66,368 learnable parameters fort the multihead attention layer\n",
        "###\n",
        "### 2. Number input or outut sequence has no contribution to learnable parameters\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers, models, Sequential, utils\n",
        "from tensorflow.keras import activations,applications, optimizers\n",
        "num_heads=4\n",
        "key_dim=64\n",
        "layerNorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "layerNorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "multiHeadAttenLayer = layers.MultiHeadAttention(\n",
        "num_heads=num_heads, key_dim=key_dim,dropout=0.1)\n",
        "dense1 = layers.Dense(units=key_dim*2, activation=tf.nn.gelu)\n",
        "dense2 = layers.Dense(units=key_dim, activation=tf.nn.gelu)\n",
        "dropout1 = layers.Dropout(0.1)\n",
        "dropout2 = layers.Dropout(0.1)\n",
        "# Layer normalization 1\n",
        "input = tf.keras.Input(shape=(145,64))\n",
        "x=input\n",
        "x1 = layerNorm1(x)\n",
        "# Multi-head attention layer\n",
        "x1 = multiHeadAttenLayer(x1, x1)\n",
        "# Skip connection 1\n",
        "x = layers.Add()([x, x1])\n",
        "\n",
        "# MLP layer\n",
        "x1 = layerNorm2(x)\n",
        "x1 = dense1(x1)\n",
        "x1 = dropout1(x1)\n",
        "x1 = dense2(x1)\n",
        "x1 = dropout2(x1)\n",
        "# Skip connection 2\n",
        "output = layers.Add()([x, x1])\n",
        "model = tf.keras.Model(input, output)\n",
        "model.summary(expand_nested=True)\n",
        "\n",
        "\n"
      ]
    }
  ]
}